"""
DrawML
Recurrent Neural Network template file

There is some issues in input data
and maybe save model
"""
import tensorflow as tf
import numpy as np
import sys
import os

training_epoch  = {{training_epoch}}
x_vertical      = {{x_vertical}}
x_horizontal    = {{x_horizontal}}
y_size          = {{y_size}}
rnn_size        = {{rnn_size}}
batch_size      = {{batch_size}}
time_step_size  = {{time_step_size}}
layer_size      = {{layer_size}}


{% include 'drawml_functions' %}



def make_model(rnn_size: int, layer_size: int, batch_size:int, time_step_size: int):
    cell_type = {{cell_type}}
    cell_module = None
    if cell_type == 'rnn':
        cell_module = tf.nn.rnn_cell.BasicRNNCell
    elif cell_type == 'lstm':
        cell_module = tf.nn.rnn_cell.BasicLSTMCell
    elif cell_type == 'gru':
        cell_module = tf.nn.rnn_cell.GRUCell

    src_cell = cell_module(rnn_size)
    cell = tf.nn.rnn_cell.MultiRNNCell([src_cell] * layer_size)

    initial_state = tf.zeros([batch_size, cell.state_size])
    x_split = tf.split(0, time_step_size, x_train)
    outputs, last_state = tf.nn.rnn(cell, x_split, initial_state)

    # logits: list of 2D Tensors of shape [batch_size x num_decoder_symbols]
    # targets: list of 1D batch-sized int32 Tensors of the same length as logits.
    # weights: list of 1D batch-sized float-Tensors of the same length as logits.
    logits = tf.reshape(tf.concat(1, outputs), [-1, rnn_size])
    targets = tf.placeholder(tf.int32, [batch_size, time_step_size])
    targets = tf.reshape(targets, [-1])
    weights = tf.ones([time_step_size * batch_size])

    return logits, targets, weights


def cost_function(logits, targets, weights, batch_size):
    cost = tf.nn.seq2seq.sequence_loss_by_example([logits], [targets], [weights])
    cost = tf.reduce_sum(cost) / batch_size
    return cost


x_train, y_train, x_valid, y_valid, x_test, y_test = load_input()

{% raw %}
with tf.device({{device}}):
{% endraw %}
    x_train = x_train.reshape(-1, x_vertical, x_horizontal, 1)
    x_test  = x_test.reshape(-1, x_vertical, x_horizontal, 1)
    x_valid = x_valid.reshape(-1, x_vertical, x_horizontal, 1)

    logits, targets, weights = make_model(rnn_size, layer_size, batch_size, time_step_size)

    cost = cost_function(logits, targets, weights, batch_size)
    optimizer = make_optimizer()
    train = optimizer.minimize(cost)
    predict = tf.argmax(logits, 1)

{% raw %}
config = tf.ConfigProto()

# default configure
config.use_per_session_threads = True

# about log
config.log_device_placement = {{log_placement}}

{% if 'cpu' in device: %}
# about cpu
config.inter_op_parallelism_threads = {{inter_threads_count}}
config.intra_op_parallelism_threads = {{intra_threads_count}}
{% else %}
# about gpu
config.allow_soft_placement = {{allow_soft_placement}},
config.gpu_options.per_process_gpu_memory_fraction = {{gpu_memory_fraction}}
{% endif %}

with tf.Session(config=config) as sess:
{% endraw %}
    init = tf.initialize_all_variables()
    sess.run(init)
    for _ in range(training_epoch):
        for start, end in zip(range(0, len(x_train), batch_size), range(batch_size, len(x_train)+1, batch_size)):
            train_data = load_train_data(start, end)
            sess.run(train, feed_dict=train_data)
        accuracy = sess.run(predict)
        print("step ", _ , " accuracy ", accuracy)

    # some logging codes will be added...
    save_model(sess, SAVE_PATH)
